{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93619a9e-3407-4736-b531-f628b35a0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53eedf-a69b-46ea-afdc-358f8b35700e",
   "metadata": {},
   "source": [
    "Spark cluster parallelism \n",
    "executors_num\n",
    "memory_per_ex\n",
    "cores_per_execut\n",
    "s = executors_num * cores_per_execut = 400 slotes\n",
    "20 block => 20 slotes ~ 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435dce30-e244-49b2-860a-46fc2f1467a7",
   "metadata": {},
   "source": [
    "# 1. HOW TO CREATE RDD\n",
    "# we can build RDDs out of local collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae08e150-e45c-4f30-8175-880e2059d4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = range(1, 1000000)\n",
    "numbers_parent_rdd = sc.parallelize(numbers, 4)\n",
    "\n",
    "# Dependency: numbers_rdd => numbers_rdd_2\n",
    "# Linage: partition: block => numbers_rdd => numbers_rdd_2\n",
    "numbers_parent_rdd.take(10)\n",
    "\n",
    "numbers_parent_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a94ed5-704e-4588-9d53-4ac5d2d3c373",
   "metadata": {},
   "source": [
    "How to read a file in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abf843d-8b55-4198-93e0-04ffd9275e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AAPL', 'Jan 1 2000', '25.94'],\n",
       " ['AAPL', 'Feb 1 2000', '28.66'],\n",
       " ['AAPL', 'Mar 1 2000', '33.95'],\n",
       " ['AAPL', 'Apr 1 2000', '31.01'],\n",
       " ['AAPL', 'May 1 2000', '21'],\n",
       " ['AAPL', 'Jun 1 2000', '26.19'],\n",
       " ['AAPL', 'Jul 1 2000', '25.41'],\n",
       " ['AAPL', 'Aug 1 2000', '30.47'],\n",
       " ['AAPL', 'Jun 1 2004', '16.27'],\n",
       " ['AAPL', 'Jul 1 2004', '16.17']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_rdd_v2 = sc.textFile(\"data/stocks/aapl.csv\"). \\\n",
    "    map(lambda row: row.split(\",\")). \\\n",
    "    filter(lambda tokens: float(tokens[2]) > 15)\n",
    "\n",
    "stocks_rdd_v2.take(10)\n",
    "\n",
    "# protected def getPartitions: Array[Partition]\n",
    "Partition -> adress block of file -> \n",
    "aapl.csv\n",
    "block 1  -> Partition\n",
    "block 2  \n",
    "block 3\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f042d2e-0888-425c-af23-444e69ff99c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25.94',\n",
       " '28.66',\n",
       " '33.95',\n",
       " '31.01',\n",
       " '21',\n",
       " '26.19',\n",
       " '25.41',\n",
       " '30.47',\n",
       " '12.88',\n",
       " '9.78']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from a DF\n",
    "stocks_df = spark.read.csv(\"data/stocks\"). \\\n",
    "    withColumnRenamed(\"_c0\", \"company\"). \\\n",
    "    withColumnRenamed(\"_c1\", \"date\"). \\\n",
    "    withColumnRenamed(\"_c2\", \"price\")\n",
    "\n",
    "stocks_rdd_v3 = stocks_df.rdd\n",
    "\n",
    "prices_rdd = stocks_rdd_v3.map(lambda row: row.price)\n",
    "prices_rdd.toDebugString()\n",
    "prices_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409efef6-edbf-46c8-986d-2358116235b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company='AAPL', date='Jan 1 2000', price='25.94'),\n",
       " Row(company='AAPL', date='Feb 1 2000', price='28.66'),\n",
       " Row(company='AAPL', date='Mar 1 2000', price='33.95'),\n",
       " Row(company='AAPL', date='Apr 1 2000', price='31.01'),\n",
       " Row(company='AAPL', date='May 1 2000', price='21'),\n",
       " Row(company='AAPL', date='Jun 1 2000', price='26.19'),\n",
       " Row(company='AAPL', date='Jul 1 2000', price='25.41'),\n",
       " Row(company='AAPL', date='Aug 1 2000', price='30.47'),\n",
       " Row(company='AAPL', date='Sep 1 2000', price='12.88'),\n",
       " Row(company='AAPL', date='Oct 1 2000', price='9.78')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD to DF\n",
    "# condition: the RDD must contain Spark Rows (data structures conforming to a schema)\n",
    "stocks_df_v2 = spark.createDataFrame(stocks_rdd_v3)\n",
    "stocks_df_v2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33257034-53b5-464f-a6eb-1888df2f5814",
   "metadata": {},
   "source": [
    "    Use cases for RDDs\n",
    "    - the computations that cannot work on DFs/Spark SQL API\n",
    "    - very custom perf optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c67ffe-ee90-41dc-87e2-9258e7b31648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'AMZN', 'MSFT', 'IBM', 'GOOG']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD transformations\n",
    "# map, filter, flatMap\n",
    "\n",
    "# distinct\n",
    "company_names_rdd = stocks_rdd_v3 \\\n",
    "    .map(lambda row: row.company) \\\n",
    "    .distinct()\n",
    "company_names_rdd.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d09cf766-1458-4285-a664-2fe6b1feda3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting\n",
    "total_entries = stocks_rdd_v3.count()  # action - the RDD must be evaluated\n",
    "total_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f724343a-67b7-41c8-b847-072d6a4280bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223.02\n",
      "7.07\n"
     ]
    }
   ],
   "source": [
    "# min and max\n",
    "aapl_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .filter(lambda row: row.company == \"AAPL\") \\\n",
    "    .map(lambda row: float(row.price))\n",
    "\n",
    "\n",
    "narrow transformation\n",
    "filter: RDD => RDD\n",
    "map: RDD => RDD\n",
    "\n",
    "action\n",
    "count: RDD => integer\n",
    "max: RDD => max\n",
    "\n",
    "\n",
    "\n",
    "max_aapl = aapl_stocks_rdd.max()\n",
    "min_aapl = aapl_stocks_rdd.min()\n",
    "print(max_aapl)\n",
    "print(min_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f1b93c-946b-45ad-ba93-6eae9dbbcd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7961.850000000001\n"
     ]
    }
   ],
   "source": [
    "# reduce ACTION\n",
    "sum_prices = aapl_stocks_rdd \\\n",
    "    .reduce(lambda x, y: x + y)  # can use ANY Python function here  1,2,3,4 => 1+2 = 3 + 3 = 6 + 4\n",
    "print(sum_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcce2031-9c25-407c-be5d-381f816b8b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AAPL', <pyspark.resultiterable.ResultIterable at 0x7fc7ee623250>),\n",
       " ('AMZN', <pyspark.resultiterable.ResultIterable at 0x7fc7ee58afa0>),\n",
       " ('MSFT', <pyspark.resultiterable.ResultIterable at 0x7fc7ef74d2e0>),\n",
       " ('IBM', <pyspark.resultiterable.ResultIterable at 0x7fc7ee59b820>),\n",
       " ('GOOG', <pyspark.resultiterable.ResultIterable at 0x7fc7ee59d040>)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping\n",
    "grouped_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .groupBy(lambda row: row.company)  # can use ANY grouping criterion as a Python function\n",
    "# grouping is expensive - involves a shuffle\n",
    "grouped_stocks_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebaa4a8-17a9-4568-8171-c455df01f47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitioning\n",
    "repartitioned_stocks_rdd = stocks_rdd_v3.repartition(4).coalesce(2) #.repartition(row.company)  #.coalesce(2)\n",
    "\n",
    "repartitioned_stocks_rdd.getNumPartitions()\n",
    "# repartitioned_stocks_rdd.getNumPartitions()\n",
    "\n",
    "# RDD\n",
    "#  part1 => |||||| 20           \n",
    "#  part2 => |||||||||||||| 40   \n",
    "#  part3 => ||||| 10            \n",
    "#  part4 => ||||| 10       \n",
    "\n",
    "\n",
    "# nums of executor * nums of cores = max_parallesim = number slot = 100\n",
    "#  I/O or filter, map, ....\n",
    ".repartition(2) # shuffle\n",
    "#  part1 => ||||| 40\n",
    "#  part2 => ||||| 40\n",
    "\n",
    "50 block => 50 partitions => 50 tasks \n",
    "50 slot will \n",
    "50 slots will \n",
    "\n",
    "# 1 GB ~  size of each row * count\n",
    "# size of each row = 1MB  => 200 MB\n",
    "\n",
    ".coalesce(5) # 5 parttions => \n",
    "# part1 => |||||| 20 + |||||||||||||| 40 => 60\n",
    "# part2 => ||||| 10 +   ||||| 10 => 20\n",
    "\n",
    ".coalesce(1)\n",
    "\n",
    "\n",
    ".repartition(col(\"company\")) # 200\n",
    "#  part1 => |||||| 20           \n",
    "#  part2 => |||||||||||||| 40   \n",
    "#  part3 => ||||| 10            \n",
    "#  part4 => ||||| 10       \n",
    "\n",
    "\n",
    "\n",
    "repartitioned_stocks_rdd.join()\n",
    "\n",
    "repartitioned_stocks_rdd.group()\n",
    "\n",
    "repartitioned_stocks_rdd.group()\n",
    "\n",
    "# getHash from \"company\" => number - 12312312312321 % 200 => rest 1-200 => 1\n",
    "# part1 => | => groups with a equal  \"company\"  \"Coca Cola\"\n",
    "# .....\n",
    "# part36 => ||| => \"BMW\"\n",
    "# .....\n",
    "# part200  => |||||||| \"VW\"\n",
    "\n",
    "groupBy = \"company\"\n",
    "join = \"company\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: SLIDE\n",
    "# .repartition(30)  # involves a shuffle\n",
    "# involves a shuffle\n",
    "#  .repartition(5) 100\n",
    "#  part1 => |||||| 20           20 2  =>\n",
    "#  part2 => |||||||||||||| 40   20 2  => |||||||||||||| 40 + |||||| 20 = 60\n",
    "#  part3 => ||||| 10            20 2\n",
    "#  part4 => |||||||||| 30       20 2  => |||||||||| 30 + ||||| 10 = 40\n",
    "#  part5 =>                     20 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d5c70-c2fa-498d-8ee5-7905ed5d0fd4",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "1. Read the movies dataset as an RDD\n",
    "\n",
    "2. Show the distinct genres as an RDD\n",
    "\n",
    "3. Print all the movies in the Drama genre with IMDB rating > 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f2e63f7-5bdf-4b24-825a-0c4121a14a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Creative_Type=None, Director=None, Distributor='Gramercy', IMDB_Rating=6.1, IMDB_Votes=1071, MPAA_Rating='R', Major_Genre=None, Production_Budget=8000000, Release_Date='12-Jun-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='The Land Girls', US_DVD_Sales=None, US_Gross=146083, Worldwide_Gross=146083),\n",
       " Row(Creative_Type=None, Director=None, Distributor='Strand', IMDB_Rating=6.9, IMDB_Votes=207, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=300000, Release_Date='7-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='First Love, Last Rites', US_DVD_Sales=None, US_Gross=10876, Worldwide_Gross=10876),\n",
       " Row(Creative_Type=None, Director=None, Distributor='Lionsgate', IMDB_Rating=6.8, IMDB_Votes=865, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=250000, Release_Date='28-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='I Married a Strange Person', US_DVD_Sales=None, US_Gross=203134, Worldwide_Gross=203134),\n",
       " Row(Creative_Type=None, Director=None, Distributor='Fine Line', IMDB_Rating=None, IMDB_Votes=None, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=300000, Release_Date='11-Sep-98', Rotten_Tomatoes_Rating=13, Running_Time_min=None, Source=None, Title=\"Let's Talk About Sex\", US_DVD_Sales=None, US_Gross=373615, Worldwide_Gross=373615),\n",
       " Row(Creative_Type='Contemporary Fiction', Director=None, Distributor='Trimark', IMDB_Rating=3.4, IMDB_Votes=165, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=1000000, Release_Date='9-Oct-98', Rotten_Tomatoes_Rating=62, Running_Time_min=None, Source='Original Screenplay', Title='Slam', US_DVD_Sales=None, US_Gross=1009819, Worldwide_Gross=1087521)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = spark.read.json(\"data/movies\")\n",
    "movies_rdd = movies_df.rdd\n",
    "\n",
    "movies_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63eaf-93ff-4e38-8277-57d64f8f883a",
   "metadata": {},
   "source": [
    "# 2. HOW TO SAVE AND PERSIST RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1df60a8-27a0-4bcd-851b-ee662773780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ints = sc.parallelize(r, 4).coalesce(2)\n",
    "\n",
    "ints.getNumPartitions()\n",
    "\n",
    "ints.saveAsTextFile(\"data/output/ints\")\n",
    "\n",
    "\n",
    "# ints = sc.parallelize(r).coalesce(1)\n",
    "# ints.coalesce(2) \\\n",
    "#     .saveAsTextFile(\"data/output/ints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "faa36640-29c9-469c-aee1-24fa5fb4e444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedInts = sc.textFile(\"data/output/ints\") \\\n",
    "    .map(lambda x: int(x)) \\\n",
    "    .persist(StorageLevel.DISK_ONLY_2) \\\n",
    "    # .cache()\n",
    "# cachedInts.count()\n",
    "\n",
    "\n",
    "# firts_res_rdd = cachedInts.join\n",
    "# firts_res_rdd.save\n",
    "\n",
    "# second_res_rdd = cachedInts.group\n",
    "# second_res_rdd.save\n",
    "\n",
    "cachedInts.take(1)\n",
    "\n",
    "cachedInts.unpersist()\n",
    "\n",
    "cachedInts.count()\n",
    "\n",
    "#  very important to count() after cashing\n",
    "# cachedInts.first()\n",
    "# cachedInts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f44eb69-e9c8-44ca-a9bb-33c1ce12780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedInts.unpersist()\n",
    "\n",
    "unpersisted = cachedInts.map(lambda x: x + 1).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40f4d75b-1ca9-44ec-ae4b-3d25c08c6b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 6, 7, 8]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = cachedInts.reduce(lambda x, y: x + y)\n",
    "# unpersisted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25760d54-42e9-4272-8be7-edb4881fcd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "Name is Even numbers id is 173\n",
      "(2) Even numbers PythonRDD[173] at collect at /tmp/ipykernel_298/2273959520.py:6 []\n",
      " |  data/output/ints MapPartitionsRDD[153] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  data/output/ints HadoopRDD[152] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# How to show plan \n",
    "\n",
    "doubles = cachedInts.map(lambda x: x * 2)\n",
    "\n",
    "even = cachedInts.filter(lambda x: x % 2 == 0)\n",
    "print(even.collect())\n",
    "\n",
    "\n",
    "even.setName(\"Even numbers\")\n",
    "print(\"Name is \" + even.name() + \" id is \" + str(even.id()))\n",
    "\n",
    "plan = even.toDebugString().decode(\"utf-8\")\n",
    "\n",
    "print(plan)\n",
    "print(doubles.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77352e87-0b25-4182-81f0-467331037c88",
   "metadata": {},
   "source": [
    "# 3. HOW TO GROUP AND JOIN RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b4956d6-0bcd-48ca-a4ff-def362d69931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 240), ('Petr', 39), ('Elena', 290), ('Elena', 300)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "codeRows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ef57bf9-bf14-4682-943e-de84b8ef7d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ivan', 240), ('Petr', 39), ('Elena', 590)]\n"
     ]
    }
   ],
   "source": [
    "# how to reduce\n",
    "reduced = codeRows.reduceByKey(lambda x, y: x + y)\n",
    "print(reduced.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8920c4c-10ee-48da-992f-9f72c81f70f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ivan', 240), ('Petr', 39), ('Elena', 300)]\n"
     ]
    }
   ],
   "source": [
    "# how to deduplicate\n",
    "deduplicated = codeRows.reduceByKey(lambda x, y: x if (x > y) else y)\n",
    "print(deduplicated.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "179d5758-6230-48c8-b88c-ad3270f2eeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 1240), ('Petr', 1039), ('Elena', 1590)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to fold by key\n",
    "folded = codeRows.foldByKey(1000, lambda x, y: x + y)\n",
    "\n",
    "\n",
    "# TODO Sliding\n",
    "#     How to foldByKey works\n",
    "#     part1 (k1:2, k2:2, k3:2, k1:2) => shufle => reduce (k1:2, k1:2, k1:2) => k1:6\n",
    "#     part2 (k2:2, k2:2, k3:2, k1:2) shufle => (k2:2, k2:2, k2:2) => k2:6, (k3:2, k3:2) => k3:4\n",
    "\n",
    "folded.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "615665f4-a0a4-4de7-9eea-1574cd829494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 1240), ('Petr', 1039), ('Elena', 1590)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregated\n",
    "aggregated = codeRows.aggregateByKey(1000, lambda x, y: x + y, lambda x, y: x + y)\n",
    "aggregated.collect()\n",
    "\n",
    "#     How to aggregateByKey works, shuffle less\n",
    "#     part1 (k1:2, k2:2, k3:2, k1:2) => (k1:4, k2:2, k3:2) =>  shuffle => (k1:4, k1:2) => k1:6\n",
    "#     part2 (k2:2, k2:2, k3:2, k1:2) => (k1:2, k2:4, k3:2) => shuffle => (k2:4, k2:2) => k2:6, (k3:2, k3:2) => k3:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06cdbceb-07ac-4b88-83d9-582b30bc82ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ivan', <pyspark.resultiterable.ResultIterable object at 0x7fc7ee232430>), ('Petr', <pyspark.resultiterable.ResultIterable object at 0x7fc7ee2322b0>), ('Elena', <pyspark.resultiterable.ResultIterable object at 0x7fc7ee2323a0>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(1) PythonRDD[290] at collect at /tmp/ipykernel_109/2934942739.py:3 []\\n |  MapPartitionsRDD[289] at mapPartitions at PythonRDD.scala:145 []\\n |  ShuffledRDD[288] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(1) PairwiseRDD[287] at groupByKey at /tmp/ipykernel_109/2934942739.py:2 []\\n    |  PythonRDD[286] at groupByKey at /tmp/ipykernel_109/2934942739.py:2 []\\n    |  ParallelCollectionRDD[235] at readRDDFromFile at PythonRDD.scala:274 []'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # groupByKey works\n",
    "grouped = codeRows.groupByKey()\n",
    "# TODO show the inner array\n",
    "print(grouped.collect())\n",
    "\n",
    "grouped.toDebugString().decode(\"utf-8\")\n",
    "\n",
    "# b'(1) PythonRDD[19] at collect at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:208 []\\n |\n",
    "# MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:145 []\\n |\n",
    "# ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "# \\n +-(1) PairwiseRDD[16] at groupByKey at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:207 []\n",
    "# \\n    |  PythonRDD[15] at groupByKey at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:207 []\n",
    "# \\n    |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'\n",
    "# # Don't forget about joins with preferred languages\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3191ca-ab7a-49ad-a98d-ec94b144dd36",
   "metadata": {},
   "source": [
    "# Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "272e6f0e-8461-4465-b2db-39c280a835c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 'Java'), ('Elena', 'Scala'), ('Petr', 'Scala')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profileData = [(\"Ivan\", \"Java\"), (\"Elena\", \"Scala\"), (\"Petr\", \"Scala\")]\n",
    "programmerProfiles = sc.parallelize(profileData)\n",
    "programmerProfiles.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64cfced5-276d-4b22-9828-c06355168c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[215] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[214] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[213] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[212] at join at /tmp/ipykernel_298/3472480033.py:3 []\n",
      "    |  PythonRDD[211] at join at /tmp/ipykernel_298/3472480033.py:3 []\n",
      "    |  UnionRDD[210] at union at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[208] at RDD at PythonRDD.scala:53 []\n",
      "    |  ParallelCollectionRDD[207] at readRDDFromFile at PythonRDD.scala:274 []\n",
      "    |  PythonRDD[209] at RDD at PythonRDD.scala:53 []\n",
      "    |  ParallelCollectionRDD[176] at readRDDFromFile at PythonRDD.scala:274 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Elena', ('Scala', 290)),\n",
       " ('Elena', ('Scala', 300)),\n",
       " ('Ivan', ('Java', 240)),\n",
       " ('Petr', ('Scala', 39))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD joining possible for only tuples\n",
    "\n",
    "joined = programmerProfiles.join(codeRows)\n",
    "print(joined.toDebugString().decode(\"utf-8\"))\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29713661-8b43-46d1-8a25-78e42fd66c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elena',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc834d90820>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc834d90850>)),\n",
       " ('Ivan',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc834d90f70>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc834d90fa0>)),\n",
       " ('Petr',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc834d91040>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc834d910a0>))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cogroup is fullouter join with dividing array\n",
    "\n",
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "codeRows = programmerProfiles.cogroup(codeRows)\n",
    "codeRows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6b9e990-a4e8-46f1-a408-ad796157f0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Petr',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc7ee280eb0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc7ed8d5ca0>)),\n",
       " ('Ivan',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc7ee232940>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc7ee232370>)),\n",
       " ('Elena',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc7ee2afaf0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc7ee2afd60>))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting\n",
    "programmerProfiles.cogroup(codeRows).sortByKey(False).collect()\n",
    "\n",
    "# TODO Write code to show inner arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "477a5557-7f6b-40ae-9b76-b42bba22a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CountByKey\n",
      "defaultdict(<class 'int'>, {'Elena': 2, 'Ivan': 1, 'Petr': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"== CountByKey\")\n",
    "print(str(joined.countByKey()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9a36613-5925-451a-9b14-35f0086fa848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Lookup\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Randomness of hash of string should be disabled via PYTHONHASHSEED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# or get all values by specific key\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== Lookup\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(\u001b[43mjoined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mElena\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:2609\u001b[0m, in \u001b[0;36mRDD.lookup\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2606\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m kv: kv[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m key)\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   2608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitioner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mrunJob(values, \u001b[38;5;28;01mlambda\u001b[39;00m x: x, [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:227\u001b[0m, in \u001b[0;36mPartitioner.__call__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k):\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumPartitions\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:91\u001b[0m, in \u001b[0;36mportable_hash\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mThis function returns consistent hash code for builtin types, especially\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mfor None and tuple with None.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m219750521\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomness of hash of string should be disabled via PYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Randomness of hash of string should be disabled via PYTHONHASHSEED"
     ]
    }
   ],
   "source": [
    "# or get all values by specific key\n",
    "print(\"== Lookup\")\n",
    "print(str(joined.lookup(\"Elena\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6901654b-8484-4b21-8d07-5f40920e05e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Keys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Elena', 'Ivan', 'Petr']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# codeRows keys only\n",
    "print(\"== Keys\")\n",
    "codeRows.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9b5ac72e-fde9-4ad2-b85b-35d1b1938ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<pyspark.resultiterable.ResultIterable at 0x7fc7ee28fca0>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7fc7ee2a9040>),\n",
       " (<pyspark.resultiterable.ResultIterable at 0x7fc7ed8d5e20>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7fc7ed8d5f70>),\n",
       " (<pyspark.resultiterable.ResultIterable at 0x7fc7edf6b460>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7fc7ee2a46a0>)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print values only\n",
    "print(\"== Value\")\n",
    "codeRows.values().take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8563ac4-6c84-4332-830f-b2563f595d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Creative_Type=None, Director=None, Distributor='Gramercy', IMDB_Rating=6.1, IMDB_Votes=1071, MPAA_Rating='R', Major_Genre=None, Production_Budget=8000000, Release_Date='12-Jun-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='The Land Girls', US_DVD_Sales=None, US_Gross=146083, Worldwide_Gross=146083), Row(Creative_Type=None, Director=None, Distributor='Strand', IMDB_Rating=6.9, IMDB_Votes=207, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=300000, Release_Date='7-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='First Love, Last Rites', US_DVD_Sales=None, US_Gross=10876, Worldwide_Gross=10876), Row(Creative_Type=None, Director=None, Distributor='Lionsgate', IMDB_Rating=6.8, IMDB_Votes=865, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=250000, Release_Date='28-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='I Married a Strange Person', US_DVD_Sales=None, US_Gross=203134, Worldwide_Gross=203134), Row(Creative_Type=None, Director=None, Distributor='Fine Line', IMDB_Rating=None, IMDB_Votes=None, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=300000, Release_Date='11-Sep-98', Rotten_Tomatoes_Rating=13, Running_Time_min=None, Source=None, Title=\"Let's Talk About Sex\", US_DVD_Sales=None, US_Gross=373615, Worldwide_Gross=373615), Row(Creative_Type='Contemporary Fiction', Director=None, Distributor='Trimark', IMDB_Rating=3.4, IMDB_Votes=165, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=1000000, Release_Date='9-Oct-98', Rotten_Tomatoes_Rating=62, Running_Time_min=None, Source='Original Screenplay', Title='Slam', US_DVD_Sales=None, US_Gross=1009819, Worldwide_Gross=1087521)]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").json(\"data/movies\")\n",
    "movies_rdd = df.rdd\n",
    "\n",
    "print(movies_rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7cc40b40-ec27-4d04-bb2e-48b3fb95c91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 'Drama', 'Comedy', 'Musical', 'Thriller/Suspense']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_movies = movies_rdd.map(lambda row: row.Major_Genre).distinct()\n",
    "dist_movies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2be1ee46-213d-4ef8-9120-89368cf5be94",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 170.0 failed 1 times, most recent failure: Lost task 0.0 in stage 170.0 (TID 173) (8c82e8c1a1d9 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_109/2136324967.py\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for &: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_109/2136324967.py\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for &: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [118]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m python_lambda_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m movie: movie\u001b[38;5;241m.\u001b[39mMajor_Genre \u001b[38;5;241m&\u001b[39m movie\u001b[38;5;241m.\u001b[39mIMDB_Rating \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      4\u001b[0m s_movies \u001b[38;5;241m=\u001b[39m movies_rdd\u001b[38;5;241m.\u001b[39mfilter(python_lambda_rdd)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ms_movies\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1568\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1565\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1567\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1568\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1570\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1571\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:1227\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;66;03m# Implementation note: This is implemented as a mapPartitions followed\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;66;03m# by runJob() in order to avoid having to pass a Python lambda into\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;66;03m# SparkContext#runJob.\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m-> 1227\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 170.0 failed 1 times, most recent failure: Lost task 0.0 in stage 170.0 (TID 173) (8c82e8c1a1d9 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_109/2136324967.py\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for &: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_109/2136324967.py\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for &: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# spark_dsl_only_df = col(\"Major_Genre\") == \"Drama\" && col(\"IMDB_Rating\") > 6\n",
    "python_lambda_rdd = lambda movie: movie.Major_Genre & movie.IMDB_Rating > 6\n",
    "\n",
    "s_movies = movies_rdd.filter(python_lambda_rdd)\n",
    "print(s_movies.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a7c667-6c5a-4201-9f52-3e8e38a790f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "\n",
    "grouped = codeRows.groupByKey()\n",
    "\n",
    "grouped.toDebugString().decode(\"utf-8\")\n",
    "\n",
    "grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd853d9-886d-4a36-8b5f-7ef58cfce7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
